# üï∑Ô∏è Guide pour Ajouter de Nouveaux Scrapers

Ce guide explique comment cr√©er de nouveaux scrapers pour r√©cup√©rer des donn√©es de matchs depuis d'autres sites web.

## üìã Structure d'un Scraper

Chaque scraper doit suivre la structure suivante:

```python
import requests
from bs4 import BeautifulSoup
from datetime import datetime

class MonScraper:
    """Description du scraper"""
    
    def __init__(self):
        self.base_url = "https://exemple.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 ...',
            'Accept-Language': 'ar,en-US;q=0.9,en;q=0.8'
        }
    
    def get_today_matches(self):
        """R√©cup√®re les matchs du jour"""
        today = datetime.now().strftime('%Y-%m-%d')
        return self.get_matches_by_date(today)
    
    def get_matches_by_date(self, date):
        """R√©cup√®re les matchs pour une date sp√©cifique"""
        matches = []
        # Votre logique de scraping ici
        return matches
    
    def _parse_match_item(self, item, date):
        """Parse un √©l√©ment de match"""
        return {
            'home_team': '',
            'away_team': '',
            'time': '',
            'date': date,
            'score': '-',
            'competition': '',
            'status': 'Scheduled',
            'is_live': False,
            'source': 'MonSite'
        }
```

## üéØ Sites Recommand√©s pour le Scraping

### 1. Filgoal.com
```python
# scrapers/filgoal_scraper.py
class FilgoalMatches:
    def __init__(self):
        self.base_url = "https://www.filgoal.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'ar'
        }
```

**Sections √† scraper:**
- Matchs du jour: `/matches`
- Matchs live: `/matches?date=today&status=live`
- Matchs par comp√©tition

### 2. Flashscore.com
```python
# scrapers/flashscore_scraper.py
class FlashscoreMatches:
    def __init__(self):
        self.base_url = "https://www.flashscore.com"
        # Note: Flashscore utilise beaucoup de JavaScript
        # Utilisez Selenium ou Playwright
```

### 3. Livescore.com
```python
# scrapers/livescore_scraper.py
class LivescoreMatches:
    def __init__(self):
        self.base_url = "https://www.livescore.com"
        # API JSON disponible
```

### 4. ESPN.com
```python
# scrapers/espn_scraper.py
class ESPNMatches:
    def __init__(self):
        self.base_url = "https://www.espn.com/soccer"
        # Utilise des endpoints API
```

## üõ†Ô∏è Outils Utiles

### BeautifulSoup (Sites Simples)
```python
from bs4 import BeautifulSoup
import requests

response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')

# Trouver des √©l√©ments
matches = soup.find_all('div', class_='match-card')
team_name = soup.find('span', class_='team').get_text(strip=True)
```

### Selenium (Sites avec JavaScript)
```python
from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
driver.get(url)

# Attendre le chargement
driver.implicitly_wait(10)

# Extraire les donn√©es
matches = driver.find_elements(By.CLASS_NAME, 'match-card')
```

### Playwright (Moderne)
```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch()
    page = browser.new_page()
    page.goto(url)
    
    # Extraire les donn√©es
    matches = page.query_selector_all('.match-card')
```

### Requests avec JSON (APIs)
```python
import requests

response = requests.get(api_url, headers=headers)
data = response.json()

matches = data['matches']
```

## üìù Format de Donn√©es Standard

Chaque match retourn√© doit avoir ce format:

```python
{
    'home_team': '√âquipe √† domicile',        # Requis
    'away_team': '√âquipe √† l\'ext√©rieur',    # Requis
    'time': '20:00',                          # Requis (format HH:MM)
    'date': '2024-01-15',                     # Requis (format YYYY-MM-DD)
    'score': '2 - 1',                         # Optionnel (- si pas de score)
    'competition': 'La Liga',                 # Requis
    'status': 'Live',                         # Requis (Scheduled/Live/Finished/etc)
    'is_live': True,                          # Requis (Boolean)
    'channels': ['beIN 1', 'ESPN'],          # Optionnel
    'competition_logo': 'url',                # Optionnel
    'home_logo': 'url',                       # Optionnel
    'away_logo': 'url',                       # Optionnel
    'fixture_id': '12345',                    # Optionnel
    'source': 'NomDuSite'                    # Requis
}
```

## üîß Int√©gration dans l'Application

### √âtape 1: Cr√©er le fichier scraper
```bash
# Cr√©er le fichier dans le dossier scrapers/
touch scrapers/nouveau_scraper.py
```

### √âtape 2: Impl√©menter la classe
```python
# scrapers/nouveau_scraper.py
class NouveauScraper:
    # Votre code ici
    pass
```

### √âtape 3: Importer dans app.py
```python
# Dans app.py
from scrapers.nouveau_scraper import NouveauScraper

# Initialiser
nouveau_scraper = NouveauScraper()

# Utiliser dans get_today_matches()
try:
    nouveau_matches = nouveau_scraper.get_today_matches()
    matches.extend(nouveau_matches)
except Exception as e:
    print(f"Erreur NouveauScraper: {e}")
```

## ‚ö†Ô∏è Bonnes Pratiques

### 1. Respecter les Sites
```python
# Ajouter un d√©lai entre les requ√™tes
import time
time.sleep(1)  # Attendre 1 seconde

# Limiter le nombre de requ√™tes
max_retries = 3
```

### 2. G√©rer les Erreurs
```python
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
except requests.RequestException as e:
    print(f"Erreur: {e}")
    return []
```

### 3. User-Agent R√©aliste
```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ar,en-US;q=0.9',
    'Referer': 'https://www.google.com/'
}
```

### 4. Cache pour Performance
```python
import json
import os
from datetime import datetime

def get_cached_matches(date):
    cache_file = f'cache/matches_{date}.json'
    if os.path.exists(cache_file):
        # V√©rifier si le cache est r√©cent (< 5 minutes)
        file_time = os.path.getmtime(cache_file)
        if time.time() - file_time < 300:
            with open(cache_file, 'r') as f:
                return json.load(f)
    return None
```

### 5. Encoding UTF-8
```python
response = requests.get(url)
response.encoding = 'utf-8'  # Pour les sites arabes
content = response.text
```

## üß™ Tester Votre Scraper

```python
# test_scraper.py
from scrapers.nouveau_scraper import NouveauScraper

scraper = NouveauScraper()
matches = scraper.get_today_matches()

print(f"Nombre de matchs: {len(matches)}")
for match in matches[:3]:  # Afficher les 3 premiers
    print(f"{match['home_team']} vs {match['away_team']} - {match['competition']}")
```

## üìö Exemples Complets

### Exemple 1: Scraper Simple avec BeautifulSoup
```python
# scrapers/simple_scraper.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime

class SimpleScraper:
    def __init__(self):
        self.base_url = "https://example-sports-site.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def get_today_matches(self):
        matches = []
        try:
            url = f"{self.base_url}/matches/today"
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                match_cards = soup.find_all('div', class_='match-card')
                
                for card in match_cards:
                    home = card.find('div', class_='home-team').get_text(strip=True)
                    away = card.find('div', class_='away-team').get_text(strip=True)
                    time_elem = card.find('span', class_='time').get_text(strip=True)
                    
                    matches.append({
                        'home_team': home,
                        'away_team': away,
                        'time': time_elem,
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'score': '-',
                        'competition': 'Unknown',
                        'status': 'Scheduled',
                        'is_live': False,
                        'source': 'SimpleScraper'
                    })
        except Exception as e:
            print(f"Erreur SimpleScraper: {e}")
        
        return matches
```

### Exemple 2: Scraper avec API JSON
```python
# scrapers/api_scraper.py
import requests
from datetime import datetime

class APIScraper:
    def __init__(self):
        self.base_url = "https://api.example-sports.com/v1"
        self.api_key = "your_api_key"
        self.headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
    
    def get_today_matches(self):
        matches = []
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            url = f"{self.base_url}/matches"
            params = {'date': today}
            
            response = requests.get(url, headers=self.headers, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                for fixture in data.get('fixtures', []):
                    matches.append({
                        'home_team': fixture['home']['name'],
                        'away_team': fixture['away']['name'],
                        'time': fixture['time'],
                        'date': today,
                        'score': f"{fixture['score']['home']} - {fixture['score']['away']}",
                        'competition': fixture['league']['name'],
                        'status': fixture['status'],
                        'is_live': fixture['status'] == 'LIVE',
                        'source': 'APIScraper'
                    })
        except Exception as e:
            print(f"Erreur APIScraper: {e}")
        
        return matches
```

## üîç D√©boguer un Scraper

### 1. Inspecter la R√©ponse HTML
```python
with open('debug.html', 'w', encoding='utf-8') as f:
    f.write(response.text)
```

### 2. Logs D√©taill√©s
```python
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

logger.debug(f"URL: {url}")
logger.debug(f"Status: {response.status_code}")
logger.debug(f"Matchs trouv√©s: {len(matches)}")
```

### 3. Tester les S√©lecteurs CSS
```python
# Dans votre navigateur (Console JavaScript):
document.querySelectorAll('.match-card')
```

## üìû Besoin d'Aide?

Si vous rencontrez des probl√®mes:
1. V√©rifiez les logs de la console
2. Inspectez la structure HTML du site cible
3. Testez votre scraper ind√©pendamment
4. Utilisez les outils de d√©veloppement du navigateur

## üéâ Contribuer

Une fois votre scraper cr√©√© et test√©:
1. Cr√©ez une Pull Request
2. Documentez votre scraper
3. Ajoutez des tests
4. Partagez avec la communaut√©!
